import cv2
import mediapipe as mp
import numpy as np
import torch
import pickle
import time
from archive.model import GestureClassifier
import os
import json
import actions


CONFIG_PATH = os.getenviron["GESTURE_AI_CONFIG"]

def normalize_keypoints(landmarks):
        keypoints = np.array([(lm.x, lm.y, lm.z) for lm in landmarks])
        keypoints -= np.mean(keypoints, axis=0)
        keypoints /= np.std(keypoints, axis=0) # TODO this might not be the best way to normalize
        return keypoints.flatten()

def get_action_from_gesture(gesture, mode, mapping):
    """Returns the callable action function for a given gesture and mode."""
    try:
        action_name = mapping[mode][gesture]
        return getattr(actions, action_name)
    except KeyError:
        print(f"âš ï¸ No action found for gesture '{gesture}' in mode '{mode}'")
        return None
    except AttributeError:
        print(f"âš ï¸ Action function '{action_name}' not found in actions.py")
        return None


# Load model
with open("label_map.pkl", "rb") as f:
    label_map = pickle.load(f)  # e.g., {0: 'open_palm', 1: 'fist', ...}

model = GestureClassifier(input_size=63, num_classes=len(label_map))
model.load_state_dict(torch.load("gesture_model.pth"))
model.eval()

# Load gesture mapping
if not os.path.exists(CONFIG_PATH):
    print("âš ï¸ No config file found, using default empty mapping.")
        
    with open(CONFIG_PATH, "r") as f:
        gesture_map = json.load(f)

# Context switching triggers
# TODO move this to gesture config file

context_triggers = {
    "three_fingers": "word_mode",
    "peace_sign": "media_mode",
    "ok_sign": "presentation_mode",
    "rock_on": "default"
}
active_context = "default"

# Initialize MediaPipe
mp_hands = mp.solutions.hands
hands = mp_hands.Hands(static_image_mode=False, max_num_hands=1, min_detection_confidence=0.3)
mp_draw = mp.solutions.drawing_utils

# Webcam loop
cap = cv2.VideoCapture(0)
cooldown = 3
last_trigger_time = 0
gesture_display = "Waiting..."

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
    result = hands.process(frame_rgb)

    if result.multi_hand_landmarks:
        for hand_landmarks in result.multi_hand_landmarks:
            mp_draw.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)

            try:
                keypoints = normalize_keypoints(hand_landmarks.landmark)
                input_tensor = torch.tensor(keypoints, dtype=torch.float32).unsqueeze(0)
                output = model(input_tensor)
                predicted_idx = torch.argmax(output, dim=1).item()
                gesture = label_map.get(predicted_idx, "unknown")

                current_time = time.time()
                gesture_display = f"{gesture} ({active_context})"

                # Handle mode switching gestures
                if current_time - last_trigger_time > cooldown:
                    if gesture in context_triggers:
                        new_context = context_triggers[gesture]
                        if new_context != active_context:
                            active_context = new_context
                            print(f"ðŸ”„ Context switched to: {active_context}")
                            last_trigger_time = current_time
                        continue

                    # Use user-defined mapping
                    action = get_action_from_gesture(gesture, active_context, gesture_map)
                    if action:
                        print(f"[ACTION] {gesture} in {active_context}")
                        action()
                        last_trigger_time = current_time

            except Exception as e:
                print(f"[ERROR] {e}")
    else:
        gesture_display = f"No hand detected ({active_context})"

    # Display gesture and context
    cv2.putText(frame, f"Gesture: {gesture_display}", (20, 40),
                cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.putText(frame, f"Mode: {active_context}", (20, 80),
                cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 0), 2)

    cv2.imshow("GesturAI", frame)

    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
